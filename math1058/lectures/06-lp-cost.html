<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Lecture 6</title>

		<meta name="description" content="Math1058">
		<meta name="author" content="Ian Hawke">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="../reveal.js/dist/reset.css">
		<link rel="stylesheet" href="../reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="../reveal.js/dist/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">

		<!-- Personal defaults -->
		<link rel="stylesheet" href="./reveal_css.css">
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section id="Title">

					<section>
						<h2 class="r-fit-text">Linear Programming and vertices</h2>
						<p>
							<ul style="list-style: none;">
								<li> Ian Hawke
								<li> Giles Richardson
							</ul>
						</p>

						<aside class="notes">
							We will look the costs and issues with the Fundamental Theorem discussed last time.
						</aside>
					</section>

				</section>

				<section id="Theorem">

					<section>

						<h2>Fundamental theorem</h2>
						<p>
							<b>Theorem:</b> Let $P = \{ x \in \mathbb{R}^n \colon A x \le b \}$ be the feasible region of the linear program $\max \{ c x \colon x \in P \}$. If $P$ is non-empty, the linear program <em>either</em> admits an optimal solution $x^*$ corresponding to a vertex of $P$, <em>or</em> is unbounded.
						</p>

						<p class="fragment" data-fragment-index=1>
							Rephrase: a linear program has either
						</p>
						<ul class="fragment" data-fragment-index=1>
							<li>
								no solutions ($P$ empty), or
							</li>
							<li>
								an unbounded solution (objective function "gets better" to infinity), or
							</li>
							<li>
								an optimal solution at a corner of $P$.
							</li>
						</ul>

						<aside class="notes">
							This recaps the fundamental theorem as introduced last time. We remember that the key step in the proof was using the dual representation, or Weyl-Minkowski, theorem, to link a point in the interior of the feasible region to vertices. We could then show that, for a non-empty and bounded problem, the optimal solution had to be linked to a convex combination of the vertices. By construction the optimal solution must therefore match that taken at one vertex (although in general there may be multiple vertices that are optimal).
						</aside>
					</section>

				</section>

				<section id="Sanity">

					<section>
						<h2>Sanity checks: interior points</h2>

						<div class="container">
							<div class="col">
								<p>
									<b>Proposition:</b> Interior points are not optimal.
								</p>
								<p class="fragment" data-fragment-index=1>
									Prove by contradiction: assume interior $x$ is optimal.
								</p>
								<p class="fragment" data-fragment-index=1>
									Take ball $B = \{ y \colon \| y - x \| < \delta \}$. $x$ interior implies $\exists \delta > 0$ such that $B \subset P$.
								</p>
								<p class="fragment" data-fragment-index=2>
									Can move within $B$ in direction $c = \nabla f$ improving objective function. Therefore $x$ is not optimal.
								</p>
							</div>
							<div class="col">
								<img style="width:668px" src="images/06_costs/lp_geom_interior_feasible.svg">
							</div>
						</div>

						<aside class="notes">
							We need to do some sanity checks on the fundamental theorem. First, are we really sure that a linear program cannot have the optimal solution within the interior of the feasible region?
							<br>
							We can show this purely from the geometric properties. We work by contradiction by assuming that there is, in fact, an optimal solution in the interior of the domain. The fact that this point lies within the interior means that it is surrounded by points <em>in all directions</em> that themselves lie within the feasible region. Therefore we can construct a <em>ball</em> - that is, a (hyper)-sphere of points that lie within the feasible region and are within a given distance of our assumed optimal solution.
							<br>
							We then consider points near to our assumed optimal point. As we have a ball of points within the feasible region, we can choose to move away from the assumed optimal point in any direction. We choose to move in the direction of the gradient of the objective function in such a way as to improve its value. This means we can find a point within the ball, and hence within the feasible region, that improves the value of the objective function. Therefore our original assumption that $x^*$ is optimal cannot be correct.
							<br>
							Of course, this construction does not and cannot say that the optimal solution has to lie at a vertex, only that it has to lie within the boundary of the feasible region.
						</aside>
					</section>

					<section>
						<h2>Sanity checks: relative facet interior</h2>

						<div class="container">
							<div class="col">
								<p>
									<b>Observation:</b> Points in the <em>relative interior</em> of a facet can be optimal.
								</p>
								<ul>
									<li>
										This requires the facet be normal to $c = \nabla f$.
									</li>
									<li>
										All vertices of the facet will be optimal.
									</li>
									<li>
										All convex combinations of the vertices will be optimal.
									</li>
									<li>
										Hence all points in the facet will be optimal.
									</li>
							</div>
							<div class="col">
								<img style="width:668px" src="images/06_costs/lp_geom_interior_facet.svg">
							</div>
						</div>

						<aside class="notes">
							The previous result - that we can always improve the objective function for a point in the interior - can fail when we get to the boundary.
							<br>
							In particular, this can happen if - and only if - the direction in which the objective function is improved, which is the gradient of the objective function, is normal to a facet. In this case the facet lies on a level surface of the objective function. If this corresponds to the optimal values (and convexity will ensure this), then every point within the facet will be optimal.
							<br>
							The terminology here is to talk about the <em>relative interior</em> of the facet. These are interior points only if we restrict ourselves to move within the facet. They are not part of the interior of the feasible region, as there is a direction (normal to the facet) that we cannot move in and remain within the feasible region.
						</aside>
					</section>

				</section>

				<section id="Costs">

					<section>

						<h2>Brute force</h2>

						<div class="container">
							<div class="col">
								\[
								\begin{array}{lcrlcl}
								x_A & + &  & x_B & \le & 10 \\
								x_A &  &  &  & \le & 7 \\
								    &  &  & x_B & \le & 5 \\
								x_A, &&& x_B & \ge & 0.
								\end{array}
								\]
								<p>
									Vertices:
								</p>
								\[
								\begin{alignat*}{4}
								v_1 & \colon & x_B & =  5 & \quad \text{and} & \quad & x_A & =  0, \\
								v_2 & \colon & x_B & =  5 & \quad \text{and} & \quad & x_A + x_B & =  10, \\
								v_3 & \colon & x_A & =  7 & \quad \text{and} & \quad & x_A + x_B & =  10, \\
								v_4 & \colon & x_A & =  7 & \quad \text{and} & \quad & x_B & =  0, \\
								v_5 & \colon & x_A & =  0 & \quad \text{and} & \quad & x_B & =  0.
								\end{alignat*}
								\]
							</div>
							<div class="col">
								<img style="width:668px" src="images/06_costs/lp_graphical_vertices.svg">
							</div>
						</div>

						<aside class="notes">
							This slide puts into practice the steps we've discussed in the past. The fundamental theorem says that the optimal solution must lie at a vertex. So we can enumerate all the vertices. That means we have to find each one. Once we have found each vertex, we can compute the objective function at each and see which is best.
							<br>
							This example is for the optimal portfolio problem which we solved earlier using the graphical method. We immediately see that this enumeration approach is not as smart as what  we did in the graphical case. We know that we don't need to consider every vertex, but only those in the direction of the gradient to the objective function that actually improves it. By enumerating all the vertices we are wasting a lot of effort. On the other hand, we don't have to put in all the effort to check whether each new vertex is in a particular direction.
							<br>
							We see that even with a small number of constraints we have a fair number of vertices. How many do we actually get?
						</aside>
					</section>

					<section>
						<h2>Combinations</h2>

						<div class="container">
							<div class="col">
								<ul>
									<li>
										In $\mathbb{R}^n$, $n$ equations define a point.
									</li>
									<li class="fragment">
										Any $n$ of the total $m$ constraints defines a vertex.
									</li>
									<li class="fragment">
										There are
										\[
										\binom{m}{n} = \frac{m!}{n! (m - n)!} \simeq \frac{m^m}{n^n (m - n)^{m-n}}
										\]
										possible vertices.
									</li>
									<li class="fragment">
										This escalates quickly.
									</li>
								</ul>
							</div>
							<div class="col">
								<img style="width:668px" src="images/06_costs/lp_graphical_vertices.svg">
							</div>
						</div>

						<aside class="notes">
							To fix a single point in a general number of dimensions we need one equation per dimension, assuming all the equations are independent.
							<br>
							So, in a linear program, we can pick any $n$ of the total $m$ constraints, force all to hold strictly as equations, and the system will define one possible vertex. Of course, there's no guarantee that all the constraints are independent. Neither is there any guarantee that every point so defined will actually be a vertex of the feasible region - we can see cases in the two dimensional plot where intersections between two constraints define sensible points which are not vertices of the feasible region. However, we need to compute them all in order to check.
							<br>
							There will be more constraints than variables. Hence we have multiple ways of choosing the $n$ constraints required. Standard combinatorics, of the type seen in (for example) the Introduction to probability course, gives us the number of ways that we can choose these equations, and hence the number of (possible) vertices we need to enumerate. We link this to its expression in terms of factorials.
							<br>
							Computing factorials is annoying. We can use Stirling's approximation to get a rough idea of the cost when the size of the linear program gets large. This is geometric, or factorial, growth. This is really bad as it grows very fast.
						</aside>

					</section>

					<section>
						<h2>Growth rates</h2>

						<div class="container">
							<div class="col">
								<table>
								<thead>
								  <tr>
								    <th>$n$</th>
								    <th>1</th>
								    <th>10</th>
								    <th>100</th>
								  </tr>
								</thead>
								<tbody>
								  <tr class="fragment" data-fragment-index=1>
								    <th>$n^2$</th>
								    <td>$1\mu$s</td>
								    <td>$100\mu$s</td>
								    <td>$10^{-2}$s</td>
								  </tr>
								  <tr class="fragment" data-fragment-index=2>
								    <th>$2^n$</th>
								    <td>$2\mu$s</td>
								    <td>$\sim 10^3 \mu$s</td>
								    <td>$\sim 10^{16}$ years</td>
								  </tr>
								  <tr class="fragment" data-fragment-index=3>
								    <th>$n^n$</th>
								    <td>$1\mu$s</td>
								    <td>$\sim 3$ hours</td>
								    <td>$\sim 10^{186}$ years</td>
								  </tr>
								</tbody>
								</table>
							</div>
							<div class="col r-stack">
								<img style="width:668px" src="images/06_costs/complexity_growth1.svg" class="fragment fade-out" data-fragment-index=1>
								<img style="width:668px" src="images/06_costs/complexity_growth2.svg" class="fragment fade-in-then-out" data-fragment-index=1>
								<img style="width:668px" src="images/06_costs/complexity_growth3.svg" class="fragment fade-in-then-out" data-fragment-index=2>
								<img style="width:668px" src="images/06_costs/complexity_growth4.svg" class="fragment fade-in" data-fragment-index=3>
							</div>
						</div>

						<aside class="notes">
							We look at how bad these growth rates are by examining two polynomial cases, where the number of things we need to compute grows linearly or quadratically, and two non-polynomial cases.
							<br>
							We assume each computation takes a microsecond. This is now a bit slow - top machines will now perform a few orders of magnitude faster - but given that each operation that we're interested in is finding a vertex, which means solving $n$ coupled equations, is not unreasonable when the number of decision variables gets large.
							<br>
							In the linear case the cost is directly proportional to the size of the system. Setting the proportionality constant to unity we see that even with a system of a size in the hundreds we will complete the computation in under a second.
							<br>
							In the quadratic case things get worse, but not unmanageably so. Again, even with a system of size in the hundreds, we will complete in under a second. It is only when the size gets into the thousands that we will start registering it.
							<br>
							In the first non-polynomial case things start of, for small systems, comparable. However, as the size of the system increases things rapidly get very expensive. By the time the size of the system is in the tens we will see it. By the time it reaches the hundreds it will take longer than the lifetime of the universe to complete.
							<br>
							The factorial growth seen in the brute-force method is even worse. Even for small systems we will have to wait hours. Systems of sizes in the hundreds are absurdly, impractically, expensive to solve.
						</aside>

					</section>

				</section>

				<section id="Summary">
					<section>
						<h1>Summary</h1>

						<ul>
							<li>
								Linear programs can be solved by
								<ul>
									<li>
										finding every vertex;
									</li>
									<li>
										computing the objective function at each;
									</li>
									<li>
										sorting the results.
									</li>
								</ul>
							</li>
							<li>
								This brute force method is totally impractical.
							</li>
						</ul>
					</section>
				</section>

			</div>

		</div>


		<script src="../reveal.js/dist/reveal.js"></script>
		<script src="../reveal.js/plugin/zoom/zoom.js"></script>
		<script src="../reveal.js/plugin/notes/notes.js"></script>
		<script src="../reveal.js/plugin/search/search.js"></script>
		<script src="../reveal.js/plugin/markdown/markdown.js"></script>
		<script src="../reveal.js/plugin/highlight/highlight.js"></script>
		<script src="../reveal.js/plugin/math/math.js"></script>
		<script src="../reveal.js/plugin/spotlight/spotlight.js"></script>
		<script src="./reveal_defaults.js"></script>


	</body>
</html>
