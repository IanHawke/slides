<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Numerical Methods for Matter in GR</title>

		<meta name="description" content="ETK, Aveiro, 2023">
		<meta name="author" content="Ian Hawke">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="../math1058/reveal.js/dist/reset.css">
		<link rel="stylesheet" href="../math1058/reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="../math1058/reveal.js/dist/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../math1058/reveal.js/plugin/highlight/monokai.css">

	<style type="text/css">
	  .reveal p {
	    text-align: left;
	  }
	  .reveal ul {
	    display: block;
	  }
	  .reveal ol {
	    display: block;
	  }

		.reveal pre code {
			font-size: 1.4em !important;
			line-height: 1.4 !important;
		}
		.container{
		    display: flex;
		}
		.col{
		    flex: 1;
		}
	</style>




	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section id="Title">

					<section data-background="../et-2022/figures/pch/strain.svg" data-background-position='right' data-background-size="50%">
						<div style="float: center; width: 50%">
							<h2>Numerical Methods for Matter in GR </h2>
							<p style="margin: 50px">
								<ul style="list-style: none;">
									<li> Ian Hawke
								</ul>
							</p>
							<p>
								<ul style="list-style: none;">
									<li> <a href="https://https://github.com/IanHawke">github.com/IanHawke</a></li>
									<li> <a href="https://orcid.org/0000-0003-4805-0309">orcid.org/0000-0003-4805-0309</a></li>
									<li> STAG, University of Southampton</li>
								</ul>
							</p>
							<p style="margin: 50px">
								<ul style="list-style: none;">
									<li> <a href="https://ianhawke.github.io/slides/et-2023">ianhawke.github.io/slides/et-2023</a> </li>
								</ul>
							</p>
						</div>
						<aside class="notes">
							This will be a rapid, surface level summary of numerical methods within the ETK, following on from the talks so far.
						</aside>
					</section>

				</section>

				<section id="Focus">

					<section>

						<div class="container">
							<div class="col">

								<h2>What matters</h2>

								$$
								G_{ab} = 8 \pi T_{ab}, \quad \textrm{where } T_{ab} = \dots 
								$$
								<ol>
									<li>
										scalar fields;
									</li>
									<li>
										simple fluids (dust, barotropes);
									</li>
									<li>
										finite $T$, real $Y_\text{x}$ fluids;
									</li>
									<li>
										electromagnetism (MHD, resistivity, ...);
									</li>
									<li>
										neutrinos, ...
									</li>
								 </ol>
								<p class="fragment" data-fragment-index=1>
									ETK:
								</p>
								<ol class="fragment" data-fragment-index=1>
									<li>
										couple $T_{ab} \leftrightarrow G_{ab}$;
									</li>
									<li>
										provides solvers for perfect MHD, constituitive interfaces;
									</li>
									<li>
										builds on spacetime infrastructure.
									</li>
								</ol>
								<p style="position:absolute; bottom:0;">
									<a href="https://arxiv.org/abs/2108.08649">Hammond+, 2205.11377.</a>
								</p>
							</div>

							<div class="col">
								<img src="../portsmouth-2023/figures/pch/rhomubTYe.svg" style="width:100%; margin:0px">
								<img src="../portsmouth-2023/figures/pch/mub_T_new.svg" style="width:100%; margin:0px">
							</div>
						</div>

						<aside class="notes">
							The ETK was designed with gravitational wave applications in mind. That means the motivating examples for including matter are astrophysical, in particular neutron star mergers. This should not be taken as representative of all research on relativistic numerical simulations with matter. There's a lot of work done in, for example, alternative theories of gravity with high energy physics applications where scalar fields are crucial, and cosmological applications where "dust" or radiation fluids are essential. However, for our motivating example of neutron star mergers that the ETK has been set up to support, we're looking at relativistic magneto-fluids at finite temperature, coupled to radiation transport.
						</aside>

					</section>

				</section>

				<section id="Shocks">

					<section>
						<div>
							<h2>Conservation</h2>

							<p>
								EFEs imply $\nabla_a T^{ab} = 0$.
							</p>
							<p>
								Pick a tetrad, $e_b^{(j)}$ to get
								$$
								\begin{aligned}
									&& \nabla_a \left[ e_b^{(j)} T^{ab} \right] &= \tfrac{1}{\sqrt{-g}} \partial_a \left( \sqrt{-g} e_b^{(j)} T^{ab} \right) = -T^{ab} \nabla_a e_b^{(j)} \\
									\implies && \partial_t {\bf q} + \partial_i {\bf f}^{(i)}({\bf q}) &= {\bf s}.
								\end{aligned}
								$$
								Balance law form.
							</p>
							<p>
								Only four equations: need other constituitive equations for, eg, EM, particle number, etc.
							</p>
							<p>
								Relations may need variables other than ${\bf q}$: <em>conservative to primitive</em> problem.
							</p>
						</div>
						<aside class="notes">
							We now know the outlines of our problem. We want a model of matter, coupled to GR, that we can write as a set of PDEs. This model will encode the detailed physics we're interested in, at the level we can simulate with our given resources. But before we try and be specific, let's look at what we can say in general.
							<br>
							We know the field equations require total stress energy conservation: the divergence of $T^{ab}$ is zero. We can use this to get <i>four</i> equations of motion. The most useful form for our purposes is to use the standard identity for the 4-divergence of a vector, which can be written as the <i>partial</i> 4-divergence of that vector, weighted by the metric determinant. To get from the stress-energy, which is a two-tensor, to a vector, we contract the free index with one member of a <i>tetrad</i>, which is a set of orthonormal (in our case) vectors that we can link to the coordinates. This then allows us to write stress energy conservation as four PDEs, one for each member of the tetrad.
							<br>
							The key point is that the only derivatives of the matter appear in total divergence form. The term on the right-hand-side which isn't in this form only involves derivatives of the tetrad, which can be written as derivatives of spacetime quantities. We write this abstractly in <i>balance law form</i>: all derivatives of the matter are total derivatives, but these <i>fluxes</i> must balance off against the <i>source terms</i> which come from the geometry.
							<br>
							If we simplify to Minkowski space in standard Cartesian coordinates then the source terms vanish and these balance laws reduce to the <i>conservation laws</i> for energy-momentum. However, we need to be careful of this interpretation (locally): if we write Minkowski spacetime in <i>spherical</i> coordinates then there will be source terms, but that certainly does not mean that energy-momentum is being transferred between the matter and the spacetime.
							<br>
							The final point is that the four conserved quantities q will not necessarilly be useful for the constituitive or closure relations that we need. In particular, the fluxes are often only easily written in terms of other variables. The key example is the equation of state, which is usually written in terms of thermodynamic potentials such as density and temperature which are not transparent from the conserved quantities. This is the conservative to primitive problem which in general is expensive and complex: rely on existing solutions (eg Reprimand).
						</aside>
					</section>

					<section>

						<h2>Shock formation</h2>
						<div class="container">
							<div class="col">
		
								<p>
									Advection equation
									$$
										\partial_t q + \partial_x (v q) = 0.
									$$
									Information moves right, speed $v$.
								</p>
								<p>
									Burgers equation
									$$
										\partial_t q + \tfrac{1}{2} \partial_x q^{2} = 0.
									$$
									Information moves right, speed $q$.
								</p>
								<p>
									Shocks form.
								</p>
							</div>
							<div class="col">
								<img src="figures/characteristics.svg" style="width:100%; margin:0px">
							</div>
						</div>
						<aside class="notes">
							We see that <i>generically</i> the matter will obey conservation laws. We expect the fluxes ${\bf f}$ to depend <i>nonlinearly</i> on the conserved variables ${\bf q}$. This immediately tells us something very important.
							<br>
							The simplest conservation law is the advection equation, where the solution is propagated to the right with constant speed $v$. For a nonlinear conservation law we can <i>locally</i> approximate it as an advection-like equation, where the solution propagates with speed $\partial_q f$. This means the propagation speed depends on the data. Depending on the form of the flux function, some gradients will steadily get steeper, and some will get shallower. In the absence of dissipation (and crucially the generic total energy-momentum equations have no dissipative terms) these steepening effects will lead to discontinuities: a generic <i>shock</i> will form.
						</aside>
					</section>

					<section>
						<div class="container">
							<div class="col">
								<h2>Nonlinearity</h2>

								<p>
									Formally need to work with <em>weak form</em>
								</p>
								$$
								\begin{aligned}
									&& \partial_t q + \nabla_k f^{(k)}(q) & = 0 \\
									\implies && \frac{\text{d}}{\text{d}t} \int_V q + \oint_{\partial V} \hat{n}_k  f^{(k)}(q) & = 0.
								\end{aligned}
								$$
								<p>
									Discrete version, 1d:
								</p>
								$$
								\frac{\text{d}}{\text{d}t} \hat{q}_i + \frac{1}{\Delta x} \left[ f_{i+1/2} - f_{i-1/2} \right] = 0.
								$$
								<p>
									Finding the flux $f_{k \pm 1/2}$ depends on the model (for fluids see <a href="https://einsteintoolkit.org/thornguide/EinsteinEvolve/GRHydro/documentation.html"><code>GRHydro</code></a> etc).
								</p>
							</div>
							<div class="col">
								<img src="../et-2022/figures/weak_solutions1.svg" style="width:100%">
							</div>
						</div>

						<aside class="notes">
							All the previous discussion has focused on linear models such as the wave and advection equations. However, matter models such as fluids naturally lead to nonlinear balance laws. The nonlinearity means that information is propagated at a speed that depends on the data, meaning that generic initial data will lead to discontinuities forming. At this point the strong form of the PDE does not make sense. Instead we need to use the weak form which follows from the finite volume representation shown earlier, where we integrate the PDE over the cell volume.
							<br>
							The telescoping flux approach here is crucial to get the right solution when shocks form. There are additional constraints on the method, but we'll see that this flux conservation step is fundamental.
						</aside>

					</section>
	
					<section>
						<div class="container">
							<div class="col">
								<h2>Consistency</h2>

								<p>
									Using the right weak form is essential.
								</p>
								$$
								\begin{aligned}
									&& \partial_t q^n + \frac{n}{n+1} \partial_x q^{n+1} & = 0 \\
									\implies && \partial_t q + q \partial_x q & = 0.
								\end{aligned}
								$$
								<p>
									Strong solutions agree when continuous; inconsistent at shocks.
								</p>
								<p class="fragment">
									Can use entropy pairs, path-consistent methods for complex cases. 
								</p>
							</div>
							<div class="col">
								<video controls data-autoplay loop="true" src="../et-2022/figures/burgers_n_shock.mp4" style="width:768px;">
								</div>
							</div>

						<aside class="notes">
							Getting shock propagation speeds right is essential and depends both on how the model is written and how the numerical method implements it. A standard example is shown on the slide, where - if we were only dealing with differentiable solutions - the results should be identical for all values of n. However, as soon as a discontinuity forms, its speed depends strongly on the value of n. This means that manipulating a balance law for numerical advantage has to be done with care, as both the model and the numerics need to be consistent.
							<br>
							The point here is that there can be multiple, even infinite, weak solutions that could in principle evolve from discontinuous data. With standard methods the choice of weak form implicitly picks out which weak solution is chosen by the numerics. We'll come back to this.
							<br>
							Finally, there is the concern of what to do when you don't have a natural balance law form, or can't write your equations as a total derivative at all. Superfluids and other multi-fluid problems have this issue. In this case you have to explicitly state what the dissipation operator is and construct a scheme to use that entropy.
						</aside>

					</section>

				</section>

				<section id="GRHydro">

					<section>
						<div class="container">
							<div class="col">
								<h2>Algorithm outline</h2>

								<p>
									Discrete version, 1d:
								</p>
								$$
								\frac{\text{d}}{\text{d}t} \hat{q}_i + \frac{1}{\Delta x} \left[ f_{i+1/2} - f_{i-1/2} \right] = 0.
								$$
								<ol>
									<li>
										<em>Reconstruct</em> $\{ \hat{q}_i \} \to q(x)$;
									</li>
									<li>
										<em>Solve</em> discontinuous problem at cell boundaries.
									</li>
								</ol>
								<br>
								<ul class="fragment">
									<li>
										Sources "just" add (stiffness?);
									</li>
									<li>
										Can reconstruct $f$ directly;
									</li>
									<li>
										Use <code>MoL</code> in time;
									</li>
									<li>
										High accuracy needs good reconstruction and time evolution.
									</li>
								</ul>
							</div>
							<div class="col">
								<img src="../et-2022/figures/weak_solutions1.svg" style="width:100%">
							</div>
						</div>

						<aside class="notes">
							The classic algorithm used by Einstein Toolkit codes works as follows.
							<br>
							The weak solution is stored as an integral average over the grid cells. Using the weak form we write the update in terms of the flux through the boundary of the cell. In one dimension that's the flux contributions into the cell from the left and out through the right.
							<br>
							We then take these integral average solutions and reconstruct a pointwise function interpolating the data. This has to ensure conservation of the data, or shocks are inconsistent. It has to allow for discontinuities: all practical methods put discontinuities at the cell boundaries. So generically we assume that the data is always discontinuous at cell boundaries.
							<br>
							We then need to solve this discontinuous problem at the boundary to compute the flux. This is usually called the <em>Riemann Problem</em>. Strictly the Riemann Problem finds the solution for all variables, and we then construct the flux from them (using self-similarity). However, we can (and often do) use approximations that give the flux directly from the variables.
							<br>
							We can then update the discrete integral averages using the flux contributions. This is often talked about as updating the reconstructed solution which is then integral averaged back onto the discrete cells.
							<br>
							Adding sources can be done algebraically. Provided they change slowly in time, which is true for the geometric terms, this is fine. To evolve in time we use the standard method of lines approach. Both absolute and order of accuracy are dominated by the reconstruction method, with some impact from the time evolution. The Riemann solver has an impact near shocks and ensures stability.
						</aside>
	
					</section>

					<section>
						<div class="container">
							<div class="col">
								<h2>Reconstruction</h2>

								<p>
									Any high order method shows Gibbs' oscillations at jumps.
								</p>
								<p>
									No convergence with resolution.
								</p>
								<p class="fragment" data-fragment-index=1>
									Use <em>piecewise</em> polynomial reconstruction (eg WENO) to avoid oscillations.
								</p>
								<ul class="fragment" data-fragment-index=2>
									<li>
										Converges with resolution.
									</li>
									<li>
										Formally first order at jumps.
									</li>
									<li>
										<em>Much</em> more expensive than linear FD.
									</li>
									<li>
										Extension to higher dimensions an issue.
									</li>
									<li>
										<em>Total variation</em> $\sup_i \sum | {\bf q}_{i+1} - {\bf q}_i |$ <em>bounded</em>.
									</li>
								</ul>
								<p class="fragment" data-fragment-index=3>
									See <a href="https://einsteintoolkit.org/thornguide/EinsteinEvolve/GRHydro/documentation.html"><code>GRHydro</code></a>, <a href="https://einsteintoolkit.org/thornguide/WVUThorns/IllinoisGRMHD/documentation.html"><code>IllinoisGRMHD</code></a>, <a href="https://www.brunogiacomazzo.org/?page_id=623"><code>Spritz</code></a>, <a href="http://personal.psu.edu/dur566/whiskythc.html"><code>WhiskyTHC</code></a>.
								</p>
							</div>
							<div class="col r-stack">
								<img src="../et-2022/figures/gibbs1.svg" style="width:100%">
								<img src="../et-2022/figures/recon21.svg" style="width:100%" class="fragment" data-fragment-index=1>
								<img src="../et-2022/figures/recon81.svg" style="width:100%" class="fragment" data-fragment-index=2>
								<img src="../et-2022/figures/gibbs2.svg" style="width:100%" class="fragment" data-fragment-index=3>
							</div>
						</div>

						<aside class="notes">
							If reconstruction dominates the accuracy of the scheme, we want to make it as accurate as possible. We'd usually do this using as much discrete data as possible with a smooth polynomial. Unfortunately that fails.
							<br>
							The standard Gibbs' effect is usually introduced in terms of Fourier series but can be seen in other function basis representations. That is, when a finite amount of data is used to represent a discontinuous jump in a field, the truncated series approximation will oscillate near that jump. When we differentiate the approximation in order to approximate the derivative of the original field, those oscillations are amplified. As Gibbs' oscillations do not converge with amplitude as the order of the truncation is increased, this means the oscillations in the approximate derivative get worse as more computational effort is used.
							<br>
							For finite difference methods these oscillations exist but are confined to a region near the jumps. As spectral methods rely on a dense differentiation matrix, the oscillations induced contaminate the entire domain.
							<br>
							The alternative that avoids, or at least minimises, the oscillations, is to change the data used in the interpolating polynomial to avoid the jump. This ensures that Gibbs' oscillations are never introduced, so the derivative is fine. However, it means that the reconstruction step now depends on the data itself. There are many ways of doing this, from second order slope limited methods to high order WENO methods. Whilst they are essential for discontinuous data they are much more costly - in some cases an order of magnitude or more - than finite differencing.
							<br>
							These sort of reconstruction methods can be found in a number of thorns in the ETK. If developing your own code then for initial tests I would recommend taking the slope limited methods from, for example, <code>GRHydro</code>, but higher order methods will be essential eventually. For those I would recommend starting with the WENO methods which are most generically implemented in <code>WhiskyTHC</code>.
							<br>
							As a theory matter, the crucial point here is that we need to constrain the total variation of the solution. Intuitively, this is linked to the maximum size of any jump in the discrete data. If this is unbounded the solutions won't converge, so this is a crucial restriction.
						</aside>

					</section>

					<section>
						<div class="container">
							<div class="col">
								<h2>Riemann Solvers</h2>
								<ul>
									<li>
										Reconstruction $\to$ Riemann Problem;
									</li>
									<li>
										Full solution as <em>waves</em>: expensive;
									</li>
									<li>
										Approximations usually enough;
									</li>
									<ul class="fragment" data-fragment-index=1>
										<li>
											Roe - linearize: 
										</li>
										<ul>
											<li>
												sharp;
											</li>
											<li>
												entropy violations.
											</li>
										</ul>
										<li>
											Marquina - fixes Roe:
										</li>
										<ul>
											<li>
												flux formula only;
											</li>
											<li>
												expensive.
											</li>
										</ul> 
										<li>
											HLL(E) - impose number of waves: 
										</li>
										<ul>
											<li>
												cheap;
											</li>
											<li>
												positive;
											</li>
											<li>
												diffusive.
											</li>
										</ul>
									</ul>
								</ul>
							</div>
							<div class="col r-stack">
								<img src="figures/RP1.svg" style="width:100%">
								<img src="figures/RP2.svg" style="width:100%" class="fragment" data-fragment-index=1>
								
							</div>
						</div>
						<aside class="notes">
							Given the reconstructed data we now need to solve for the flux at cell boundaries. We can approximate the problem at cell boundaries as being piecewise constant - this is accurate enough. This is known as the Riemann Problem, where we need to know how the solution evolves in time.
							<br>
							In this sketch picture we see the density evolving with time, splitting into three waves: rarefaction, contact, and shock. We rely on the self-similarity of the problem: from the solution along the axis we have the solution for all future time, from which we compute the flux.
							<br>
							Unfortunately, finding the full solution with all waves correct is far too expensive - try using Bruno's solver for MHD to see the cost. Also it's not needed in evolution codes, as approximations are enough.
							<br>
							Most approximations to the solution look at the waves in parameter space - here the x axis is still space but the vertical axis is time. The wave structure shows the self-similarity: each line is straight, and the solution between waves is constant. The real cost in finding the exact solution is iteratively finding the correct wave structure, so most approximations start with that wave structure. Here I note three approaches, all seen in GRHydro.
							<br>
							For Roe's solver we linearize the nonlinear flux (in particular, its Jacobian) about some state. That makes the problem linear and hence easy to compute. It gives very sharp discontinuous waves but isn't good at continuous waves and can fail horribly when a transsonic rarefaction appears.
							<br>
							Marquina fixes Roe by checking for transsonic rarefactions and replacing the flux computed then. This is highly accurate, but is expensive and only gives the flux directly, without going through the solution.
							<br>
							Finally, the least accurate but most robust solution comes from the HLL family of solvers. In this case the wave speeds are over-estimated and only a limited number of waves are used. We can then enforce conservation to get the solution. This is the standard solver to start with - it can smear out shocks, but with good reconstruction methods is often enough. HLL also guarantees that certain quantities that you want to remain positive (like density) do so.
						</aside>
					</section>

					<section>
						<div class="container">
							<div class="col">
								<h2>Entropy functions</h2>

								<p>
									Entropy $\eta({\bf q})$ is convex function, flux $\psi$, s.t.
								</p>
								$$
								\partial_t \eta + \partial_x \psi \le 0, \quad \nabla_{\bf q} \psi = \nabla_{\bf q} \eta \nabla_{\bf q} {\bf f}.
								$$
								<p>
									Entropy picks out <em>unique</em> weak solution.
								</p>
								<p>
									Given two solutions ${\bf q}_{1, 2}$, then
								</p>
								$$
								\frac{d}{dt} \| {\bf q}_{1} - {\bf q}_{2} \| \le 0
								$$
								<p>
									<em>only</em> if both are entropy solutions.
								</p>
								<p class="fragment">
									Conclusion: if numerical scheme violates entropy condition, it can and will diverge from true solution.
								</p>
							</div>
							<div class="col">
								<img src="figures/entropy_violation1.svg" style="width:70%">
							</div>
						</div>
						<aside class="notes">
							Finally, an underlying point is entropy. I've noted that Roe has problems for transsonic cases. I've noted that multiple weak solutions can be consistent with given initial data. So the question is how we enforce that we get the right solution.
							<br>
							The sketch on the right illustrates the problem. We have a Riemann problem with separating data, as seen from the characteristic speeds. We expect to get the rarefaction solution at the top. This is because if we perturbed the initial data very slightly, smoothing it out, then the characteristic speeds should vary in exactly this way. However, both of the weak solutions below are consistent with the discontinuous data. They do something very odd - information appears out of the shock discontinuously - but mathematically they are fine.
							<br>
							To fix this problem we have to find an entropy pair: a scalar function of the conserved quantities that is convex, called the entropy, and its associated flux. This pair satisfies a conservation law, where the entropy strictly decreases only at shocks. Yes, decreases: blame numerical analysts for this choice.
							<br>
							Entropy solutions satisfy the conservation laws and the entropy law. If our numerical solution also obeys the entropy law then it will converge to some solution of the real problem. In essence, entropy solutions enforce that characteristics also propagate into shocks, not out of them.
							<br>
							Now, Murphy's law says that if we don't enforce that our numerical scheme obeys the entropy condition then it violates it. The corollary to the above is that an entropy violating solution will not converge to any solution of the original problem. So entropy consistency is essential.
							<br>
							It turns out that a consitent entropy stable flux (so, not Roe), combined with the telescoping flux form, enforces an entropy solution. Combine that with total variation boundedness and we can be confident that we're getting the right thing. However, the approaches can be overly restrictive.
						</aside>
					</section>

				</section>

				<section id="MHD">

					<section>
						<div class="container">
							<div class="col">
								<h2>MHD</h2>
								<ul>
									<li>
										Still conservation laws (ideal case).
									</li>
									<li>
										Constraint $\mathcal{C} = \nabla \cdot {\bf B} \equiv 0$ essential.
									</li>
									<li>
										Can either
									</li>
									<ul>
										<li>add damping terms $\propto - \mathcal{C}$</li>
										<li>enforce $\mathcal{C}=0$ discretely.</li>
									</ul>
									<li class="fragment" data-fragment-index="1">
										Using vector potential does the latter - preferred.
									</li>
									<li class="fragment" data-fragment-index="2">
										Entropy function evolution has term $\propto \mathcal{C}$
									</li>
								</ul>
								<p style="position:absolute; bottom:-100px; right:50px" class="fragment fade-out" data-fragment-index="1"><a href="https://www.sciencedirect.com/science/article/abs/pii/0021999180900790">Brackbill and Barnes</a></p>
								<p style="position:absolute; bottom:-100px; right:50px" class="fragment" data-fragment-index="1"><a href="https://ui.adsabs.harvard.edu/abs/2020CQGra..37m5010C">Cipolletta+</a></p>
							</div>
							<div class="col r-stack">
								<img src="../icts-2020/figures/brackbill_barnes.png" style="width:100%" class="fragment fade-out" data-fragment-index="1">
								<img src="../icts-2020/figures/GRID_CELL.png" style="width:100%"  class="fragment" data-fragment-index="1">
							</div>
						</div>
						<aside class="notes">
							Finally, we need to discuss the additional issues that come in when we add physics beyond just perfect fluids. We'll focus on MHD. In this case the Maxwell equations are easily written as balance laws, but there is the no monopole divergence constraint as well.
							<br>
							It's been known for a very long time that ensuring the divergence constraint holds is crucial - the figure here is from Brackbill and Barnes from  1980. Since then a number of approaches have been tried. In constraint damping terms are added proportional to the constraint (which should vanish) so that the evolution of the error converges to zero exponentially. Every time numerical error kicks the divergence away from zero, the damping term forces it back.
							<br>
							The alternative approach is to enforce the constraint at the discrete level. This should be compared to the way that a telescoping flux enforces conservation at the discrete level. One method, used by IllinoisGRMHD and Spritz, is to evolve a vector potential stored at cell edges. The magnetic field is then the curl of the vector potential, which is divergence free by construction.
							<br>
							It's worth noting that entropy pairs have been constructed for MHD (in general, and for relativistic MHD for simple equations of state). With this entropy pair we see that the evolution of the entropy has a term that is proportional to the divergence constraint. The proportionality constant is nonlinear and complex. Again, Murphy's law says that unless we explicitly enforce otherwise, a violation of the divergence constraint will make the entropy grow with the wrong sign, and lead to convergence to the wrong solution.
						</aside>
					</section>

				</section>

				<section id="Summary">
					<section>

						<div class="container">
							<div class="col">
								<h2>Summary</h2>

								<p>
									ETK matter simulations
								</p>
								<ul>
									<li>
										focus on finite volume approaches;
									</li>
									<li>
										are "good enough" (?) for LIGO, but...
									</li>
									<li>
										are costly, inaccurate (compared to spacetime).
									</li>
								</ul>
								<p>
									Focus on shocks important, but restricts accuracy.
								</p>
								<p>
									Long term accuracy needs better approaches (DG, compact FD will help).
								</p>
								<p>
									Fast/short effects need LES-type treatment.
								</p>
								<p>
									Entropy stability only non-negotiable.
								</p>
								<p style="position:absolute; bottom:50px; left:500px">
									<a href="https://arxiv.org/abs/2108.08649">Hammond+</a>; <a href="http://arxiv.org/abs/2207.00442">Most+</a>.
								</p>
							</div>
							<div class="col">
								<img src="../et-2022/figures/pch/adaptive_model.png" style="height:960px">
							</div>
						</div>

						<aside class="notes">
							The steps I've outline above form the basis of the astrophysical ETK evolution codes involving matter. They've been good enough for now, but the costs are high and the accuracy low compared to spacetime codes. For next generation detectors we need order of magnitude improvements.
							<br>
							A lot of the focus in the field has been ensuring correct evolution at shocks. This is essential, but shocks form a tiny region of the spacetime. By indirectly enforcing entropy stability and TVB at every point in spacetime we are restricting accuracy. Methods that focus on different metrics for accuracy such as compact differencing schemes could give significant improvements.
							<br>
							Finally, we should note that matter evolution, particularly through merger, can see lots of structure and power built up on short lengthscales and fast timescales, below the scales that we can resolve on the grid. There's a lot of effort right now to capture this physics using LES-type approaches, which are naturally ad hoc. However, ensuring stability with these ad hoc terms need analysis that hasn't yet been done: I believe entropy stability should be the non-negotiable basis on which we build future schemes.
							<br>
							Thank you for your attention.
						</aside>

					</section>
				</section>

				<section id="Further material">
					<section>
						<h2>Further reading</h2>
						<ul>
							<li>
								<a href="https://doi.org/10.1007/lrca-2015-3">Marti & Müller, Grid-based Methods in Relativistic Hydrodynamics and Magnetohydrodynamics, Living Review</a>. SR only but updated in 2015. See also the <a href="https://link.springer.com/article/10.12942/lrr-2003-7">original version from 2003</a>.
							</li>
							<li>
								<a href="https://doi.org/10.1007/s41115-017-0002-8">Balsara, Higher-order accurate space-time schemes for computational astrophysics—Part I: finite volume methods, Living Review</a>. Very methods heavy. Cutting edge but not easy going.
							</li>
							<li>
								<a href="https://www.cambridge.org/core/books/finite-volume-methods-for-hyperbolic-problems/97D5D1ACB1926DA1D4D52EAD6909E2B9">Leveque, Finite Volume Methods for Hyperbolic Problems, CUP</a>. No astrophysics but one of the standard numerical methods texts.
							</li>
							<li>
								<a href="https://epubs.siam.org/doi/book/10.1137/1.9781611975109">Hesthaven, Numerical Methods for Conservation Laws: From Analysis to Algorithms, SIAM</a>. Still no astrophysics and even more mathematical-technical, but goes deep into methods like Discontinuous Galerkin.
							</li>
							<li>
								<a href="https://open-astrophysics-bookshelf.github.io/">Open Astrophysics Bookshelf</a>. Relativity isn't a focus but the material covers a lot of numerics in great depth, with example codes throughout. Have a look at <a href="https://github.com/python-hydro">github.com/python-hydro</a> for detailed examples in one and two dimensions.
							</li>
							<li>
								<a href="https://www.sciencedirect.com/science/article/pii/S1570865916300436">Uncertainty Quantification</a>. A review of a crucial, but very difficult, task for the next generation of simulations.
							</li>
						</ul>
					</section>
				</section>

			</div>

		</div>


		<script src="../math1058/reveal.js/dist/reveal.js"></script>
		<script src="../math1058/reveal.js/plugin/zoom/zoom.js"></script>
		<script src="../math1058/reveal.js/plugin/notes/notes.js"></script>
		<script src="../math1058/reveal.js/plugin/search/search.js"></script>
		<script src="../math1058/reveal.js/plugin/markdown/markdown.js"></script>
		<script src="../math1058/reveal.js/plugin/highlight/highlight.js"></script>
		<script src="../math1058/reveal.js/plugin/math/math.js"></script>
		<script src="../math1058/reveal.js/plugin/spotlight/spotlight.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
				width: 1536,
			  height: 960,
			  margin: 0.04,
				controls: false,
				progress: true,
				center: true,
				hash: true,
				transition: 'none',
				pdfSeparateFragments: true,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealMath.KaTeX, RevealSpotlight  ],
				spotlight: {
					presentingCursor: 'default',
					useAsPointer: true,
					size: 10,
					toggleSpotlightOnMouseDown: false,
					spotlightOnKeyPressAndHold: true,
				},
				keyboard: {
					// alternative to toggleSpotlightOnMouseDown:
					// toggle spotlight by pressing key 'c'
					67: function() { RevealSpotlight.toggleSpotlight() },
				},
			});

		</script>


	</body>
</html>
